{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85d7015-9411-45d1-bc94-8892097f5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import zlib\n",
    "from pynbt import NBTFile\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def translate_chunk_location(x, z, h_offset):\n",
    "    # Calculate the chunk coordinates\n",
    "    c_x = h_offset % 32\n",
    "    c_z = h_offset // 32\n",
    "\n",
    "    # Now, `offset` is the location of the chunk's data in the file,\n",
    "    # and `x` and `z` are the chunk's coordinates within this region.\n",
    "    # print(f\"Chunk at [{x}({c_x}), {z}({c_z})] has an offset of {h_offset}\")\n",
    "    x_min = (x * 16) + (c_x * 16)\n",
    "    x_max = (x * 16) + (c_x * 16) + 16\n",
    "    z_min = (z * 16) + (c_z * 16)\n",
    "    z_max = (z * 16) + (c_z * 16) + 16\n",
    "\n",
    "    # print(f\"Chunk covers [{x_min}, {z_min}] [{x_min}, {z_max}] [{x_max}, {z_min}] [{x_max}, {z_max}]\")\n",
    "\n",
    "    return c_x, c_z\n",
    "\n",
    "\n",
    "def get_chunk_offset_and_length(header, i):\n",
    "    # Extract the four bytes for this chunk entry\n",
    "    entry_bytes = header[i * 4: i * 4 + 4]\n",
    "\n",
    "    # The first three bytes are the offset (big-endian)\n",
    "    offset = int.from_bytes(entry_bytes[:3], 'big')\n",
    "\n",
    "    # The fourth byte is the length\n",
    "    length = entry_bytes[3]\n",
    "\n",
    "    return offset, length\n",
    "    \n",
    "\n",
    "def read_region_file(filepath):\n",
    "    # Get the filename from path.\n",
    "    filename = filepath.split('/')[-1]\n",
    "\n",
    "    # Filename format is r.x.z.mca\n",
    "    filesplit = filename.split('.')\n",
    "\n",
    "    # X and Z coords are parts 1 and 2 of the filename.\n",
    "    x = int(filesplit[1])\n",
    "    z = int(filesplit[2])\n",
    "    \n",
    "    chunks = {}\n",
    "    filename = f'data/region/r.{x}.{z}.mca'\n",
    "    with open(filename, 'rb') as file:\n",
    "        # Region files begin with an 8192 byte header\n",
    "        header = file.read(8192)\n",
    "\n",
    "        # locations (1024 entries)\n",
    "        for i in range(0, 1024):\n",
    "            offset, length = get_chunk_offset_and_length(header, i)\n",
    "            \n",
    "            if offset == 0 and length == 0:\n",
    "                continue  # Chunk is not present\n",
    "                \n",
    "            # Convert the offset to bytes (multiply by 4096)\n",
    "            offset *= 4096\n",
    "\n",
    "            # check past offset\n",
    "            file.seek(0, 2)  # end to of file\n",
    "            if file.tell() < offset: \n",
    "                # offset is past file, no chunk exists\n",
    "                continue\n",
    "\n",
    "            # goto offset and read \n",
    "            file.seek(offset)\n",
    "            chunk_header = file.read(5)  # Read chunk length and compression type\n",
    "            if len(chunk_header) < 5:\n",
    "                print(f\"Incomplete chunk header for chunk {i}.\")\n",
    "                continue\n",
    "\n",
    "            chunk_length, compression_type = struct.unpack('>IB', chunk_header)\n",
    "            chunk_length -= 1  # Subtract the compression type byte\n",
    "\n",
    "            # Read and decompress the chunk data\n",
    "            compressed_chunk_data = file.read(chunk_length)\n",
    "            if len(compressed_chunk_data) < chunk_length:\n",
    "                print(f\"Incomplete chunk data for chunk {i}.\")\n",
    "                continue\n",
    "\n",
    "            uncompressed_chunk = zlib.decompress(compressed_chunk_data)\n",
    "            chunks[i] = {\n",
    "                'nbt': uncompressed_chunk,\n",
    "                'i':  i,\n",
    "                'offset': offset,\n",
    "                'chunk_length': len(compressed_chunk_data),\n",
    "                'uncompressed_chunk_length': len(uncompressed_chunk),\n",
    "                'compression_type': compression_type,\n",
    "            }\n",
    "    return chunks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a547625-ea28-4571-aafc-508407a57b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_block_section(block_states_section, block_type_full_palette):\n",
    "    # Need size of palette to calculate bit-size\n",
    "    palette_size = len(block_states_section['palette'])\n",
    "\n",
    "    # Iterate over the palette to add them to the full palette\n",
    "    for palette in block_states_section['palette']:\n",
    "        # If there is no entry for the palette then add one\n",
    "        block_type_full_palette.setdefault(palette['Name'].value, len(block_type_full_palette))\n",
    "\n",
    "    # If there is only 1 entry in the palette\n",
    "    # Tne entire section is that entry\n",
    "    if palette_size == 1:\n",
    "        # Create an array that contains the single value from the palette\n",
    "        block_type_array = np.full((16,16,16), block_type_full_palette[block_states_section['palette'][0]['Name'].value])\n",
    "        return block_type_array\n",
    "\n",
    "    # Bit size is 4 or least number of bits required to represent largest index \n",
    "    bit_size = max(4, math.ceil(math.log2(palette_size)))\n",
    "    \n",
    "    # Create an empty array to store the block states\n",
    "    block_type_array = np.empty((16,16,16), dtype=int)\n",
    "\n",
    "    \n",
    "    # element = block_states_section['data'].value[0]\n",
    "    # print(block_states_section['data'])\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Get block data\n",
    "    for element in block_states_section['data'].value:\n",
    "        # We start reading from 0 \n",
    "        bits_read = 0\n",
    "        \n",
    "        # Keep reading until we get to 64 bits\n",
    "        while bits_read + bit_size <= 64:\n",
    "            # Shift element to the bits_read, then mask so we only read bit_size\n",
    "            index = (element >> bits_read) & ((1 << bit_size) - 1)\n",
    "            # Insert into the array the index of the palette from the full biome palette\n",
    "            # Convert current idx into yzx coords\n",
    "            yxz_coords = get_coords(current_idx, 16)\n",
    "            # The index we want is the index of the palette from the full palette\n",
    "            full_palette_index = block_type_full_palette[block_states_section['palette'][index]['Name'].value]\n",
    "            # Store the value of the index in the multi dim array\n",
    "            try:\n",
    "                block_type_array[yxz_coords] = full_palette_index\n",
    "            except Exception as e:\n",
    "                break\n",
    "                print(e)\n",
    "                print(f\"current_idx={current_idx}\")\n",
    "                print(f\"yxz_coords={yxz_coords}\")\n",
    "            # Step forward to read next bits\n",
    "            bits_read += bit_size\n",
    "            # Next idx\n",
    "            current_idx += 1\n",
    "            \n",
    "        if current_idx >= 4095:\n",
    "            break\n",
    "    \n",
    "    return block_type_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f82a89f-ca07-4ea6-9cf4-f5b012a85263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_biome_section(biome_section, biome_full_palette):\n",
    "    # Need size of palette to calculate bit-size\n",
    "    palette_size = len(biome_section['palette'])\n",
    "\n",
    "    # Iterate over the palette to add them to the full palette\n",
    "    for palette in biome_section['palette']:\n",
    "        # If there is no entry for the palette then add one\n",
    "        biome_full_palette.setdefault(palette.value, len(biome_full_palette))\n",
    "\n",
    "    # If there is only 1 entry in the palette\n",
    "    # Tne entire section is that entry\n",
    "    if palette_size == 1:\n",
    "        biome_array = np.full((4,4,4), biome_full_palette[biome_section['palette'][0].value])\n",
    "        return biome_array\n",
    "\n",
    "    # Bit size is least number of bits required to represent largest index \n",
    "    bit_size = math.ceil(math.log2(palette_size)) \n",
    "\n",
    "    indices = []\n",
    "    # Create an empty array to store the biome\n",
    "    biome_array = np.empty((4,4,4), dtype=int)\n",
    "    current_idx = 0\n",
    "\n",
    "    # Get biome data\n",
    "    for element in biome_section['data'].value:\n",
    "        # We start reading from 0 \n",
    "        bits_read = 0\n",
    "        \n",
    "        # Keep reading until we get to 64 bits\n",
    "        while bits_read + bit_size <= 64:\n",
    "            # Shift element to the bits_read, then mask so we only read bit_size\n",
    "            index = (element >> bits_read) & ((1 << bit_size) - 1)\n",
    "            indices.append(index)\n",
    "            # Convert current idx into yzx coords\n",
    "            yxz_coords = get_coords(current_idx, 4)\n",
    "            # The index we want is the index of the palette from the full palette\n",
    "            full_palette_index = biome_full_palette[biome_section['palette'][index].value]\n",
    "            # Store the value of the index in the multi dim array\n",
    "            biome_array[yxz_coords] = full_palette_index\n",
    "            \n",
    "            # Step forward to read next bits\n",
    "            bits_read += bit_size\n",
    "            # Next idx\n",
    "            current_idx += 1\n",
    "        \n",
    "            # # Insert into the array the index of the palette from the full biome palette\n",
    "            # biome_array[get_coords(idx, 4)] = biome_full_palette[biome_section['palette'][index].value]\n",
    "            # bits_read += bit_size\n",
    "    \n",
    "        biomes = [biome_section['palette'][index].value for index in indices]\n",
    "    \n",
    "    return biome_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e58b61-b227-45d7-8db3-27a01b71ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(idx, section_size):\n",
    "    y = idx % section_size\n",
    "    z = (idx // section_size) % section_size\n",
    "    x = idx // (section_size * section_size)\n",
    "    return y, z, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d39805f-f5a0-4242-95c0-24387bd43f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(the_chunk, biome_full_palette, block_full_palette):\n",
    "    # convert to bytes to io stream and read\n",
    "    chunk_data = io.BytesIO(the_chunk)\n",
    "    nbt = NBTFile(chunk_data)\n",
    "\n",
    "    chunk_dict = {}\n",
    "\n",
    "    new_section = None\n",
    "    new_selection_idx = None\n",
    "    \n",
    "    #print(nbt.pretty())\n",
    "    \n",
    "    for idx, section in enumerate(nbt['sections']):\n",
    "        sub_chunk = read_block_section(section['block_states'], block_full_palette)\n",
    "\n",
    "    \n",
    "\n",
    "    return nbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e04234-eef5-4df7-a33e-49a61b905c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mca_file(new_chunks, file_path):\n",
    "    compressed_chunks = {}\n",
    "    current_offset = 8192 # chunk starting offset\n",
    "\n",
    "    with open(file_path, 'wb+') as file:\n",
    "        # write header\n",
    "        for i in range(0, 1024):\n",
    "            if i in new_chunks:\n",
    "                byte_stream = io.BytesIO()\n",
    "                new_chunks[i].save(byte_stream)\n",
    "                chunk_bytes = byte_stream.getvalue()\n",
    "                compressed_chunk = zlib.compress(chunk_bytes)\n",
    "                # save the chunk for future writing\n",
    "                compressed_chunks[current_offset] = compressed_chunk\n",
    "                # (5 bytes(length+compression_type) + chunk_length) / 4096\n",
    "                sector_count = math.ceil(5+ (len(compressed_chunk)) / 4096)\n",
    "                sector_offset = current_offset // 4096\n",
    "                chunk_location = struct.pack('>I', (sector_offset << 8) | sector_count)\n",
    "                file.write(chunk_location)\n",
    "                current_offset += sector_count * 4096\n",
    "            else:\n",
    "                file.write(b'\\x00' * 4)  # write 0 for location+sector count (4 bytes) \n",
    "\n",
    "        # write data\n",
    "        for chunk_offset, chunk_data in compressed_chunks.items():\n",
    "            file.seek(chunk_offset)\n",
    "            packed_header = struct.pack('>IB', len(chunk_data)+1, 2)\n",
    "            file.write(packed_header)\n",
    "            file.write(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fb81b2-ed15-4003-bcf3-11111202b6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-Copy1.-1.-1.mca\n",
      "Loaded 288 chunks.\n"
     ]
    }
   ],
   "source": [
    "# fnames = os.listdir('data/region/')\n",
    "\n",
    "fname = 'r-Copy1.-1.-1.mca'\n",
    "\n",
    "all_chunks = {}\n",
    "nbt_chunks = {}\n",
    "biome_full_palette = {}\n",
    "block_full_palette = {}\n",
    "\n",
    "print(fname)\n",
    "file_chunks = read_region_file(f'data/region/{fname}')\n",
    "print(f\"Loaded {len(file_chunks)} chunks.\")\n",
    "all_chunks.update(file_chunks)\n",
    "\n",
    "for c_idx, a_chunk in all_chunks.items():\n",
    "    nbt_chunks[c_idx] = read_chunk(a_chunk['nbt'], biome_full_palette, block_full_palette)\n",
    "\n",
    "create_mca_file(nbt_chunks, 'data/region/test.mca')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
